{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Amazon Lookout for Equipment** - Getting started\n",
    "*Part 2 - Dataset creation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "---\n",
    "This repository is structured as follow:\n",
    "\n",
    "```sh\n",
    ". lookout-equipment-demo\n",
    "|\n",
    "├── data/\n",
    "|   ├── interim                          # Temporary intermediate data\n",
    "|   ├── processed                        # Finalized datasets\n",
    "|   └── raw                              # Immutable original data\n",
    "|\n",
    "├── getting_started/\n",
    "|   ├── 1_data_preparation.ipynb\n",
    "|   ├── 2_dataset_creation.ipynb               <<< THIS NOTEBOOK <<<\n",
    "|   ├── 3_model_training.ipynb\n",
    "|   ├── 4_model_evaluation.ipynb\n",
    "|   ├── 5_inference_scheduling.ipynb\n",
    "|   ├── 6_visualization_with_quicksight.ipynb\n",
    "|   └── 7_cleanup.ipynb\n",
    "|\n",
    "└── utils/\n",
    "    └── lookout_equipment_utils.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook configuration update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade pip\n",
    "!pip install --quiet --upgrade sagemaker lookoutequipment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import config\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# SDK / toolbox for managing Lookout for Equipment API calls:\n",
    "import lookoutequipment as lookout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA = os.path.join('..', 'data', 'processed', 'getting-started')\n",
    "TRAIN_DATA     = os.path.join(PROCESSED_DATA, 'training-data')\n",
    "\n",
    "ROLE_ARN       = sagemaker.get_execution_role()\n",
    "REGION_NAME    = boto3.session.Session().region_name\n",
    "DATASET_NAME   = config.DATASET_NAME\n",
    "BUCKET         = config.BUCKET\n",
    "PREFIX         = config.PREFIX_TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookout_dataset = lookout.LookoutEquipmentDataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    component_root_dir=f's3://{BUCKET}/{PREFIX}',\n",
    "    access_role_arn=ROLE_ARN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method encapsulate the [**CreateDataset**](https://docs.aws.amazon.com/lookout-for-equipment/latest/ug/API_CreateDataset.html) API:\n",
    "\n",
    "```python\n",
    "lookout_client.create_dataset(\n",
    "    DatasetName=self.dataset_name,\n",
    "    \n",
    "    # Optional\n",
    "    DatasetSchema={\n",
    "        'InlineDataSchema': \"schema\"\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookout_dataset.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is now created, but it is empty and ready to receive some timeseries data that we will ingest from the S3 location prepared in the previous notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dataset created](assets/dataset-created.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest data into a dataset\n",
    "---\n",
    "Let's double check the values of all the parameters that will be used to ingest some data into an existing Lookout for Equipment dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE_ARN, BUCKET, PREFIX, DATASET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the ingestion job in the Lookout for Equipment dataset: the following method encapsulates the [**StartDataIngestionJob**](https://docs.aws.amazon.com/lookout-for-equipment/latest/ug/API_StartDataIngestionJob.html) API:\n",
    "\n",
    "```python\n",
    "lookout_client.start_data_ingestion_job(\n",
    "    DatasetName=DATASET_NAME,\n",
    "    RoleArn=ROLE_ARN, \n",
    "    IngestionInputConfiguration={ \n",
    "        'S3InputConfiguration': { \n",
    "            'Bucket': BUCKET,\n",
    "            'Prefix': PREFIX,\n",
    "            'KeyPattern': \"string\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lookout_dataset.ingest_data(BUCKET, PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ingestion is launched. With this amount of data (around 50 MB), it should take between less than 5 minutes:\n",
    "\n",
    "![dataset_schema](assets/dataset-ingestion-in-progress.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following cell to monitor the ingestion process by calling the following method, which encapsulates the [**DescribeDataIngestionJob**](https://docs.aws.amazon.com/lookout-for-equipment/latest/ug/API_DescribeDataIngestionJob.html) API and runs it every 60 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookout_dataset.poll_data_ingestion(sleep_time=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case any issue arise, you can inspect the API response available as a JSON document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookout_dataset.ingestion_job_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ingestion should now be complete as can be seen in the console:\n",
    "\n",
    "![Ingestion done](assets/dataset-ingestion-done.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting sensor data quality\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now inspect the data quality of your dataset by clicking on `View dataset`. In this new screen, you will be able to visualize:\n",
    "* Your dataset details with a summary of their grade. In our case, 22 sensors are marked as **High quality* while 8 sensors are marked as **Medium quality**\n",
    "* The total number of sensors ingested\n",
    "* The overall date range\n",
    "* The location of the data source on S3\n",
    "\n",
    "You then have a table with a row for each sensor where you can see the overall date range, the number of days of available data and the sensor grade. Hovering your mouse over a given sensor grade will give you the explanations linked to this grading. In the example below, you can see that Sensor0 was graded as Medium because multiple operating modes are detected. You will be able to use every sensors ingested, but the Lookout for Equipment console gives you some pieces of advice and warns about situations where bad performance may arise further down the road. To read about all the sensor grades the service checks out, [follow this link](https://docs.aws.amazon.com//lookout-for-equipment/latest/ug/reading-details-by-sensor.html):\n",
    "\n",
    "![Ingestion done](assets/dataset-inspection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can obtain these detailed information by querying the [ListSensorStatistics](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/lookoutequipment.html#LookoutEquipment.Client.list_sensor_statistics) API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = lookout_dataset.ingestion_job_response['JobId']\n",
    "\n",
    "response = lookout_dataset.client.list_sensor_statistics(DatasetName=DATASET_NAME, IngestionJobId=job_id)\n",
    "results = response['SensorStatisticsSummaries']\n",
    "while 'NextToken' in response:\n",
    "    response = l4e_client.list_sensor_statistics(DatasetName=DATASET_NAME, IngestionJobId=job_id, NextToken=response['NextToken'])\n",
    "    results.extend(response['SensorStatisticsSummaries'])\n",
    "    \n",
    "stats_df = pd.json_normalize(results, max_level=1)\n",
    "print(stats_df.shape)\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all the characteristics you can get for each sensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we created a **Lookout for Equipment dataset** and ingested the S3 data previously uploaded into this dataset. **Move now to the next notebook to train a model based on these data.**"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
