{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11597aaa",
   "metadata": {},
   "source": [
    "# **Amazon Lookout for Equipment** - Getting started\n",
    "*Part 2 - Dataset creation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af18063",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "---\n",
    "This repository is structured as follow:\n",
    "\n",
    "```sh\n",
    ". lookout-equipment-demo\n",
    "|\n",
    "├── data/\n",
    "|   ├── interim                          # Temporary intermediate data are stored here\n",
    "|   ├── processed                        # Finalized datasets are usually stored here\n",
    "|   |                                    # before they are sent to S3 to allow the\n",
    "|   |                                    # service to reach them\n",
    "|   └── raw                              # Immutable original data are stored here\n",
    "|\n",
    "├── getting_started/\n",
    "|   ├── 1_data_preparation.ipynb\n",
    "|   ├── 2_dataset_creation.ipynb         <<< THIS NOTEBOOK <<<\n",
    "|   ├── 3_model_training.ipynb\n",
    "|   ├── 4_model_evaluation.ipynb\n",
    "|   ├── 5_inference_scheduling.ipynb\n",
    "|   └── 6_cleanup.ipynb\n",
    "|\n",
    "└── utils/\n",
    "    └── lookout_equipment_utils.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b9b7c6",
   "metadata": {},
   "source": [
    "### Notebook configuration update\n",
    "Amazon Lookout for Equipment being a very recent service, we need to make sure that we have access to the latest version of the AWS Python packages. If you see a `pip` dependency error, check that the `boto3` version is ok: if it's greater than 1.17.48 (the first version that includes the `lookoutequipment` API), you can discard this error and move forward with the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed96944",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade sagemaker tqdm\n",
    "\n",
    "import boto3\n",
    "print(f'boto3 version: {boto3.__version__} (should be >= 1.17.48 to include Lookout for Equipment API)')\n",
    "\n",
    "# Restart the current notebook to ensure we take into account the previous updates:\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e537ad1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf8c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Helper functions for managing Lookout for Equipment API calls:\n",
    "sys.path.append('../utils')\n",
    "import lookout_equipment_utils as lookout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA = os.path.join('..', 'data', 'processed', 'getting-started')\n",
    "TRAIN_DATA     = os.path.join(PROCESSED_DATA, 'training-data')\n",
    "\n",
    "ROLE_ARN       = sagemaker.get_execution_role()\n",
    "REGION_NAME    = boto3.session.Session().region_name\n",
    "DATASET_NAME   = config.DATASET_NAME\n",
    "BUCKET         = config.BUCKET\n",
    "PREFIX         = config.PREFIX_TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4830f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the directories from the training data \n",
    "# directory: each directory corresponds to a subsystem:\n",
    "components = []\n",
    "for root, dirs, files in os.walk(f'{TRAIN_DATA}'):\n",
    "    for subsystem in dirs:\n",
    "        if subsystem != '.ipynb_checkpoints':\n",
    "            components.append(subsystem)\n",
    "        \n",
    "components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db68440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Timestamp', 'Tag1', 'Tag2', 'Tag3'])\n",
    "df = df.append({'Timestamp': '2019-01-01', 'Tag1': 0.25, 'Tag2': 1.1, 'Tag3': -3.14}, ignore_index=True)\n",
    "df.to_csv(file1, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf372a",
   "metadata": {},
   "source": [
    "## Create a dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bddc34",
   "metadata": {},
   "source": [
    "### Create data schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c02302",
   "metadata": {},
   "source": [
    "First we need to setup the schema of your dataset. In the cell below, we define `DATASET_COMPONENT_FIELDS_MAP`. `DATASET_COMPONENT_FIELDS_MAP` is a Python dictonary (hashmap). The key of each entry in the dictionary is the `Component` name, and the value of each entry is a list of column names. The column names must exactly match the header in your CSV files. The order of the column names also need to exactly match:\n",
    "\n",
    "```json\n",
    "DATASET_COMPONENT_FIELDS_MAP = {\n",
    "    \"Component1\": ['Timestamp', 'Tag1', 'Tag2',...],\n",
    "    \"Component2\": ['Timestamp', 'Tag1', 'Tag2',...]\n",
    "    ...\n",
    "    \"ComponentN\": ['Timestamp', 'Tag1', 'Tag2',...]\n",
    "}\n",
    "```\n",
    "\n",
    "We also need to make sure the component name **matches exactly** the name of the folder in S3 (everything is **case sensitive**). As an example, when creating the data schema for the example we are using here, we will build a the dictionary that will look like this:\n",
    "```json\n",
    "DATASET_COMPONENT_FIELDS_MAP = {\n",
    "    \"centrifugal-pump\": ['Timestamp', 'Sensor0', 'Sensor1',... , 'Sensor29']\n",
    "}\n",
    "```\n",
    "The following cell builds this map, then convert it into a JSON schema that follows the following format, which is ready to be processed by Lookout for Equipment:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Components\": [\n",
    "    {\n",
    "      \"ComponentName\": \"centrifugal-pump\",\n",
    "      \"Columns\": [\n",
    "        {\"Name\": \"Timestamp\", \"Type\": \"DATETIME\"},\n",
    "        {\"Name\": \"Sensor0\", \"Type\": \"DOUBLE\"},\n",
    "        {\"Name\": \"Sensor1\", \"Type\": \"DOUBLE\"},\n",
    "        {\"Name\": \"Sensor2\", \"Type\": \"DOUBLE\"},\n",
    "        {\"Name\": \"Sensor3\", \"Type\": \"DOUBLE\"},\n",
    "          \n",
    "        ...\n",
    "          \n",
    "        {\"Name\": \"Sensor29\", \"Type\": \"DOUBLE\"}\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c730be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "def create_data_schema_from_dir(root_dir):\n",
    "    \"\"\"\n",
    "    Generates a data schema compatible for Lookout for Equipment from a local directory\n",
    "    \n",
    "    PARAMS\n",
    "    ======\n",
    "        root_dir: string\n",
    "            A path pointing to the root directory where all the CSV files are located\n",
    "            \n",
    "    RETURNS\n",
    "    =======\n",
    "        schema: string\n",
    "            A JSON-formatted string ready to be used as a schema for an L4E dataset\n",
    "    \"\"\"\n",
    "    # List of the directories from the training data \n",
    "    # directory: each directory corresponds to a subsystem:\n",
    "    components = []\n",
    "    for _, dirs, _ in os.walk(root_dir):\n",
    "        for subsystem in dirs:\n",
    "            if subsystem != '.ipynb_checkpoints':\n",
    "                components.append(subsystem)\n",
    "\n",
    "    # Loops through each subdirectory found in the root dir:\n",
    "    DATASET_COMPONENT_FIELDS_MAP = dict()\n",
    "    for subsystem in components:\n",
    "        subsystem_tags = ['Timestamp']\n",
    "        for root, _, files in os.walk(f'{root_dir}/{subsystem}'):\n",
    "            for file in files:\n",
    "                fname = os.path.join(root, file)\n",
    "                current_subsystem_df = pd.read_csv(fname, nrows=1)\n",
    "                subsystem_tags = subsystem_tags + current_subsystem_df.columns.tolist()[1:]\n",
    "\n",
    "            DATASET_COMPONENT_FIELDS_MAP.update({subsystem: subsystem_tags})\n",
    "            \n",
    "    schema = create_data_schema(DATASET_COMPONENT_FIELDS_MAP)\n",
    "    \n",
    "    return schema\n",
    "\n",
    "def create_data_schema(component_fields_map: Dict):\n",
    "    \"\"\"\n",
    "    Generates a JSON formatted string from a dictionary\n",
    "    \n",
    "    PARAMS\n",
    "    ======\n",
    "        component_fields_map: dict\n",
    "            A dictionary containing a field maps for the dataset schema\n",
    "            \n",
    "    RETURNS\n",
    "    =======\n",
    "        schema: string\n",
    "            A JSON-formatted string ready to be used as a schema for a dataset\n",
    "    \"\"\"\n",
    "    schema = json.dumps(\n",
    "        _create_data_schema_map(\n",
    "            component_fields_map=component_fields_map\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return schema\n",
    "\n",
    "def _create_data_schema_map(component_fields_map: Dict):\n",
    "    \"\"\"\n",
    "    Generate a dictionary with the JSON format expected by Lookout for Equipment\n",
    "    to be used as the schema for a dataset at ingestion, training time and\n",
    "    inference time\n",
    "    \n",
    "    PARAMS\n",
    "    ======\n",
    "        component_fields_map: dict\n",
    "            A dictionary containing a field maps for the dataset schema\n",
    "\n",
    "    RETURNS\n",
    "    =======\n",
    "        data_schema: dict\n",
    "            A dictionnary containing the detailed schema built from the original\n",
    "            dictionary mapping\n",
    "    \"\"\"\n",
    "    # Build the schema for the current component:\n",
    "    component_schema_list = [_create_component_schema(\n",
    "            component_name, \n",
    "            component_fields_map[component_name]\n",
    "        ) for component_name in component_fields_map\n",
    "    ]\n",
    "    \n",
    "    # The root of the schema is a \"Components\" tag:\n",
    "    data_schema = dict()\n",
    "    data_schema['Components'] = component_schema_list\n",
    "\n",
    "    return data_schema\n",
    "\n",
    "def _create_component_schema(component_name: str, field_names: List):\n",
    "    \"\"\"\n",
    "    Build a schema for a given component and fieds list\n",
    "    \n",
    "    PARAMS\n",
    "    ======\n",
    "        component_name: string\n",
    "            Name of the component to build a schema for\n",
    "        \n",
    "        field_names: list of strings\n",
    "            Name of all the fields included in this component\n",
    "            \n",
    "    RETURNS\n",
    "    =======\n",
    "        component_schema: dict \n",
    "            A dictionnary containing the detailed schema for a given component\n",
    "    \"\"\"\n",
    "    if len(field_names) == 0:\n",
    "        raise Exception(f'Field names for component {component_name} should not be empty')\n",
    "    if len(field_names) == 1:\n",
    "        raise Exception(f'Component {component_name} must have at least one sensor beyond the timestamp')\n",
    "        \n",
    "    # The first field is a timestamp:\n",
    "    col_list  = [{'Name': field_names[0], 'Type': 'DATETIME'}]\n",
    "    \n",
    "    # All the others are float values:\n",
    "    col_list = col_list + [\n",
    "        {'Name': field_name, 'Type': 'DOUBLE'} \n",
    "        for field_name in field_names[1:]\n",
    "    ]\n",
    "    \n",
    "    # Build the schema for this component:\n",
    "    component_schema = dict()\n",
    "    component_schema['ComponentName'] = component_name\n",
    "    component_schema['Columns'] = col_list\n",
    "            \n",
    "    return component_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = create_data_schema_from_dir(TRAIN_DATA)\n",
    "\n",
    "pp = pprint.PrettyPrinter(depth=5)\n",
    "pp.pprint(eval(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fb68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_input = {\n",
    "    'component1': ['Timestamp'],\n",
    "    'component2': ['Timestamp', 'Tag4', 'Tag5', 'Tag6']\n",
    "}\n",
    "create_data_schema(schema_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742af6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_expected_output = {\n",
    "    'Components': [\n",
    "        {\n",
    "            'ComponentName': 'component1',\n",
    "            'Columns': [\n",
    "                {'Name': 'Timestamp', 'Type': 'DATETIME'},\n",
    "                {'Name': 'Tag1', 'Type': 'DOUBLE'},\n",
    "                {'Name': 'Tag2', 'Type': 'DOUBLE'},\n",
    "                {'Name': 'Tag3', 'Type': 'DOUBLE'}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'ComponentName': 'component2',\n",
    "            'Columns': [\n",
    "                {'Name': 'Timestamp', 'Type': 'DATETIME'},\n",
    "                {'Name': 'Tag4', 'Type': 'DOUBLE'},\n",
    "                {'Name': 'Tag5', 'Type': 'DOUBLE'},\n",
    "                {'Name': 'Tag6', 'Type': 'DOUBLE'}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "schema_expected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dab8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output == schema_expected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3bcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COMPONENT_FIELDS_MAP = dict()\n",
    "for subsystem in components:\n",
    "    subsystem_tags = ['Timestamp']\n",
    "    for root, _, files in os.walk(f'{TRAIN_DATA}/{subsystem}'):\n",
    "        for file in files:\n",
    "            fname = os.path.join(root, file)\n",
    "            current_subsystem_df = pd.read_csv(fname, nrows=1)\n",
    "            subsystem_tags = subsystem_tags + current_subsystem_df.columns.tolist()[1:]\n",
    "\n",
    "        DATASET_COMPONENT_FIELDS_MAP.update({subsystem: subsystem_tags})\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=5)\n",
    "pp.pprint(eval(create_data_schema(DATASET_COMPONENT_FIELDS_MAP)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7955af",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "lookout_dataset = lookout.LookoutEquipmentDataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    component_fields_map=DATASET_COMPONENT_FIELDS_MAP,\n",
    "    region_name=REGION_NAME,\n",
    "    access_role_arn=ROLE_ARN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a789013",
   "metadata": {},
   "source": [
    "If you wanted to use the console, the following string would be the one to use to configure the **dataset schema**:\n",
    "\n",
    "![Dataset creation with schema](assets/dataset-schema.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2264a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=5)\n",
    "pp.pprint(eval(lookout_dataset.dataset_schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61416848",
   "metadata": {},
   "source": [
    "### Create the dataset\n",
    "The following method encapsulate the [**CreateDataset**](https://docs.aws.amazon.com/lookout-for-equipment/latest/ug/API_CreateDataset.html) API:\n",
    "\n",
    "```python\n",
    "lookout_client.create_dataset(\n",
    "    DatasetName=self.dataset_name,\n",
    "    DatasetSchema={\n",
    "        'InlineDataSchema': \"schema\"\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookout_dataset.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2260451",
   "metadata": {},
   "source": [
    "The dataset is now created, but it is empty and ready to receive some timeseries data that we will ingest from the S3 location prepared in the previous notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d071d1",
   "metadata": {},
   "source": [
    "![Dataset created](assets/dataset-created.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81bd120",
   "metadata": {},
   "source": [
    "## Ingest data into a dataset\n",
    "---\n",
    "Let's double check the values of all the parameters that will be used to ingest some data into an existing Lookout for Equipment dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff41ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE_ARN, BUCKET, PREFIX, DATASET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa2f12f",
   "metadata": {},
   "source": [
    "Launch the ingestion job in the Lookout for Equipment dataset: the following method encapsulates the [**StartDataIngestionJob**](https://docs.aws.amazon.com/lookout-for-equipment/latest/ug/API_StartDataIngestionJob.html) API:\n",
    "\n",
    "```python\n",
    "lookout_client.start_data_ingestion_job(\n",
    "    DatasetName=DATASET_NAME,\n",
    "    RoleArn=ROLE_ARN, \n",
    "    IngestionInputConfiguration={ \n",
    "        'S3InputConfiguration': { \n",
    "            'Bucket': BUCKET,\n",
    "            'Prefix': PREFIX\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lookout_dataset.ingest_data(BUCKET, PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e8f62",
   "metadata": {},
   "source": [
    "The ingestion is launched. With this amount of data (around 50 MB), it should take between less than 5 minutes:\n",
    "\n",
    "![dataset_schema](assets/dataset-ingestion-in-progress.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf363420",
   "metadata": {},
   "source": [
    "We use the following cell to monitor the ingestion process by calling the [**DescribeDataIngestionJob**](https://docs.aws.amazon.com/lookout-for-equipment/latest/ug/API_DescribeDataIngestionJob.html) API every 60 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e76887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ingestion job ID and status:\n",
    "data_ingestion_job_id = response['JobId']\n",
    "data_ingestion_status = response['Status']\n",
    "\n",
    "# Wait until ingestion completes:\n",
    "print(\"=====Polling Data Ingestion Status=====\\n\")\n",
    "lookout_client = lookout.get_client(region_name=REGION_NAME)\n",
    "print(str(pd.to_datetime(datetime.now()))[:19], \"|\", data_ingestion_status)\n",
    "\n",
    "while data_ingestion_status == 'IN_PROGRESS':\n",
    "    time.sleep(60)\n",
    "    describe_data_ingestion_job_response = lookout_client.describe_data_ingestion_job(JobId=data_ingestion_job_id)\n",
    "    data_ingestion_status = describe_data_ingestion_job_response['Status']\n",
    "    print(str(pd.to_datetime(datetime.now()))[:19], \"|\", data_ingestion_status)\n",
    "    \n",
    "print(\"\\n=====End of Polling Data Ingestion Status=====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be64b6",
   "metadata": {},
   "source": [
    "In case any issue arise, you can inspect the API response available as a JSON document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookout_client.describe_data_ingestion_job(JobId=data_ingestion_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73021816",
   "metadata": {},
   "source": [
    "The ingestion should now be complete as can be seen in the console:\n",
    "\n",
    "![Ingestion done](assets/dataset-ingestion-done.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f81ee9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70595a8",
   "metadata": {},
   "source": [
    "In this notebook, we created a **Lookout for Equipment dataset** and ingested the S3 data previously uploaded into this dataset. **Move now to the next notebook to train a model based on these data.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
